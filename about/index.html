<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Stats | Performance data</title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="http://benchmarker.blackbird.pw/about/">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><meta name="author" content="Aleksandr Drozd">
<meta property="og:site_name" content="Performance data">
<meta property="og:title" content="Stats">
<meta property="og:url" content="http://benchmarker.blackbird.pw/about/">
<meta property="og:description" content="About this resource


Motivation


The variety of CPUs, GPUs, FPGAs, and now an explosion of new AI hardware accelerators
is making the field of high performance machine learning hardware increasingly">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-06-02T09:39:34Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md navbar-dark bg-dark static-top mb-4"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="http://benchmarker.blackbird.pw/">

            <span id="blog-title">Performance data</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../about" class="nav-link">about</a>
                </li>
<li class="nav-item">
<a href="https://github.com/undertherain/benchmarker/" class="nav-link">source code</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!--Body content--><article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header></header><div class="e-content entry-content" itemprop="articleBody text">
    <section class="container"><h1>About this resource</h1>


<h2>Motivation</h2>

<p>
The variety of CPUs, GPUs, FPGAs, and now an explosion of new AI hardware accelerators
is making the field of high performance machine learning hardware increasingly diverse.
At the same time multiple deep learning frameworks exist, each providing unique
performance/productivity trade-offs.
Finally, the deep learning models themselves is the area in which true Cambrian explosion
is happening. From convolutional nets, to recurrent nets to attention-based models
to autoencoders to GANs to all sorts of previously unknown exotic beasts,
new models emerge almost every day.
</p>

<p>
Which hardware is good for which models and is it properly supported by your favourite framework?
This resource is our attempt to put together some statistics on performance of
deep learning models implemented with different frameworks on a set of hardware platforms.
</p>

<h2>What are we measuring</h2>
<p>
First of all, let us clarify what is deep learning performance.
For the user the ultimate parameter is the time needed to train a model.
[this part is mostly relevant to training, for inference things are bit simpler]
</p>

<p>
This time essentially depends on two factors: how many steps are needed to
converge and how much time is needed to compute one step (sample/mini-batch/etc)

Convergence rate does, with few exceptions, depend on a hardware platform.
More importantly, optimizing for converge rate involves two many special tricks like
learning rate scheduling, switching between different optimizers etc, so that maintaining same training
regime across all frameworks is problematic.
For this reason we focus on time to iterate over a training set with default SGD optimizer and fixed learning rate.

One possible issues with this approach is that potentially a given platform could
enable faster computation in terms of samples per second at the cost of weaker convergence rate.
Most typical case would be the use of reduced arithmetic precision.
We address this issue in several ways. First of
</p>

<h2>Few words on arithmetic precision</h2>

<h2>Fairness</h2>
We area doing our best to give equal amount of love to all hardware and
software vendors.
If you notice that your device or framework or model is dramatically under-performing,
and it happens because of some mistake on our side: don't hesitate to contact.
The code is open-source, we are hoping to include installation scripts as well soon.
On the other hand, we tried to put a reasonable effort into installing every given piece properly.
If it worked like that in our settings, it is quite likely that it will work like that for most of the users :-p


<h2>One more thing...</h2>
Donâ€™t draw quick conclusions based on just single benchmark.
For almost any framework we could find regimes where it performs better
or worse than others.

Rather then comparing, let's say, frameworks against each other it seems to to be more sensible to talk about "state of
implemented-ness / optimized-ness" with respect to particular kernels/topologies for each framework/hw platform.

</section>
</div>
    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script></article><!--End of body content--><div class="container" id="content" role="main">
    <div class="body-content">

        <footer id="footer">
            Maintained by  <a href="http://blackbird.pw">Aleksandr Drozd</a>         
            
        </footer>
</div>
</div>


        <script src="../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script defer src="https://use.fontawesome.com/releases/v5.0.10/js/all.js" integrity="sha384-slN8GvtUJGnv6ca26v8EzVaR9DC58QEwsIk9q1QXdCU8Yu8ck/tL/5szYlBbqmS+" crossorigin="anonymous"></script>
</body>
</html>
